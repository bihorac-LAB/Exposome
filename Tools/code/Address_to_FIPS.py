import pandas as pd
import os
import subprocess
import sys
import argparse
from loguru import logger
from sqlalchemy import create_engine
import concurrent.futures
import zipfile
import shutil
from datetime import datetime
import gc
import platform
import shutil
import re
import shlex

# -------------------------------------------------------------------
# A quick-lookup set of FULL normalized hospital addresses.
# Add / update as needed.  All entries must be lower-case and trimmed.
HOSPITAL_ADDRESSES = {
    "1000 peachtree park dr ne atlanta ga 30309",
    "240 nw 25th st miami fl 33127",
    "1400 briarcliff rd ne atlanta ga 30306",
    # …extend the list …
}
# -------------------------------------------------------------------
host_base = os.environ["HOST_PWD"] 

# logger.add(sys.stderr, format="{time} {level} {message}", filter="my_module", level="INFO")
#get the log file
def configure_logging(output_folder):
    # Create log subfolder inside the output folder
    log_folder = os.path.join(output_folder, "log")
    os.makedirs(log_folder, exist_ok=True)

    # Create log filename based on current timestamp
    log_filename = f"address_to_fips_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    log_file_path = os.path.join(log_folder, log_filename)

    # Ensure the output folder exists
    os.makedirs(output_folder, exist_ok=True)
    logger.remove()  # Remove any previous handlers (reset the logger)

    # Configure logger to output to both the console and the log file
    logger.add(sys.stderr, format="{time} {level} {message}", level="DEBUG")  # Logs everything to terminal
    logger.add(log_file_path, format="{time} {level} {message}", level="DEBUG")  # Logs everything to file
    logger.info(f"Log file configured: {log_file_path}")

    return log_file_path  # Return the log file path for reference if needed

#Generate latitude and longitude from address infomation
def generate_coordinates_degauss(df, columns, threshold, output_folder):
    """
    Preprocess address data, execute a Docker-based geocoding tool, and retrieve geolocation data using Degauss.
    
    Parameters:
    df (pandas.DataFrame): DataFrame containing address data
    columns (list of str): List of column names representing address information
    threshold (float): Threshold for the geocoder's score (accuracy)
    save_intermediate (bool): Whether to save the intermediate preprocessed CSV before geocoding (default is False)
    
    Returns:
    str: Name of the geocoded CSV file generated by the Docker container
    """
    output_file_name = os.path.join(output_folder, f"preprocessed_1_geocoder_3.3.0_score_threshold_{threshold}.csv")
    if os.path.exists(output_file_name):
        logger.info(f"Geocoded file already exists, skipping geocoding: {output_file_name}")
        return os.path.abspath(output_file_name)
    logger.info("Generating coordinates...")

    orig_df = df.copy(deep=True)
    orig_df["_rid"] = orig_df.index
    df["_rid"]      = orig_df["_rid"]

    if "zip" in orig_df.columns:
        orig_df["zip"] = (
            orig_df["zip"]
            .fillna("")                # keep dtype object
            .astype(str)
            .str.replace(r"\.0$", "", regex=True)  # drop trailing '.0'
            .str.strip()
        )
    
    # Convert columns to string type
    for col in columns:
        df[col] = df[col].fillna("").astype(str)
    for col in columns:
         # If this is the Zip column, ensure no '.0' remains
        if col.lower() == 'zip':
            df[col] = df[col].apply(lambda x: x.split('.')[0] if '.' in x else x)
    
    # Handle single column or concatenate multiple columns
    if len(columns) == 1:
        df['address'] = df[columns[0]].str.title().replace(r'[^a-zA-Z0-9 ]', ' ', regex=True)
    else:
        df['address'] = df.apply(lambda row: ' '.join(row[columns]).lower(), axis=1)
        df['address'] = df['address'].str.title()
        df['address'] = df['address'].replace(r'[^a-zA-Z0-9 ]', ' ', regex=True)

    # Reorder columns to ensure 'address' is the first column
    cols = ['address'] + [col for col in df.columns if col != 'address']
    df = df[cols]
    
    # Drop original address columns if they are no longer needed
    if len(columns) > 1:
        df.drop(columns=columns, inplace=True)
    
    # Save the preprocessed DataFrame to CSV
    preprocessed_file_path = os.path.join(output_folder, 'preprocessed_1.csv')
    df.to_csv(preprocessed_file_path, index=False)

    # Convert the folder and file paths to absolute paths for Docker
    abs_output_folder = os.path.abspath(output_folder)  # Convert to absolute path
    abs_preprocessed_file = os.path.abspath(preprocessed_file_path)  # Convert to absolute path

    container_cwd = os.getcwd()  # This will be /workspace when using -w /workspace

    # Calculate the relative path from the container's working directory
    host_folder = os.path.join(host_base, os.path.relpath(abs_output_folder, container_cwd))
    
    # Define the Docker command
    # NOTE: binding a host directory onto container /tmp can break R's fifo() on
    # certain host filesystem mounts (causes "cannot create fifo ... Operation not supported").
    # To avoid this, mount the host folder into the container under /workspace
    # so that the container's native /tmp remains usable for temporary files.
    mount_dest = 'exposome_tmp'
    docker_command = [
        'docker', 'run', '--rm',
        '-v', f'{host_folder}:/workspace/{mount_dest}',
        'ghcr.io/degauss-org/geocoder:3.3.0',
       f'/workspace/{mount_dest}/{os.path.basename(abs_preprocessed_file)}',
        str(threshold)
    ]
    logger.debug(f"Mounting host folder to container at /workspace/{mount_dest} instead of /tmp to preserve container /tmp semantics")
    
    try:
        result = subprocess.run(' '.join(docker_command), shell=True, check=True, capture_output=True, text=True)
        logger.info("Docker command executed successfully.")
        logger.info(result.stdout)
    except subprocess.CalledProcessError as e:
        logger.error(f"Error executing Docker command: {e}")
        logger.error(e.stderr)
    try:
        # 1️⃣  read the geocoder output
        geocoded_df = pd.read_csv(output_file_name)

        # 2️⃣  bring the original address pieces back in (needed for the reason logic)
        merge_cols = [c for c in ("street", "city", "state", "zip") if c in orig_df.columns]
        geocoded_df = (
            geocoded_df
            .merge(orig_df[merge_cols + ["_rid"]], on="_rid", how="left")
            .sort_values("_rid")
            .reset_index(drop=True)
        )

        # ── helpers ────────────────────────────────────────────────
        MISSING_SENTINELS = {"nan", "na", "n/a", "none", "null", ""}

        def _blank(x: object) -> bool:
            return pd.isna(x) or str(x).strip().lower() in MISSING_SENTINELS

        def _has_coords(row) -> bool:
            """Return True **only** if lat/lon are real numbers inside
            normal geographic limits (and not NaN)."""
            try:
                lat = float(row.get("lat", ""))
                lon = float(row.get("lon", ""))
            except (ValueError, TypeError):
                return False
            if pd.isna(lat) or pd.isna(lon):
                return False
            return -90.0 <= lat <= 90.0 and -180.0 <= lon <= 180.0

        def _zip_clean(z):
            z = str(z).strip().lower()
            return z[:-2] if z.endswith(".0") else z

        def _reason(row):
            # ----- Successful geocode – hospital address test -----
            if row["geocode_result"] == "Geocoded":
                if {"street", "city", "state", "zip"}.issubset(row.index):
                    full_addr = " ".join(
                        [
                            str(row.get("street", "")).strip().lower(),
                            str(row.get("city", "")).strip().lower(),
                            str(row.get("state", "")).strip().lower(),
                            _zip_clean(row.get("zip", "")),
                        ]
                    ).strip()
                else:  # single-column case
                    full_addr = (
                        str(row.get("address", ""))
                        .lower()
                        .strip()
                    )
                return "Hospital address given" if full_addr in HOSPITAL_ADDRESSES else ""

            # ----- Imprecise geocode – why? -----
            if all(_blank(row.get(c, "")) for c in ("street", "city", "state", "zip")):
                return "Blank/Incomplete address"
            if _blank(row.get("zip", "")):
                return "Zip missing"
            if _blank(row.get("street", "")):
                return "Street missing"
            return ""

        # 3️⃣  rebuild geocode_result & reason
        geocoded_df.drop(columns=["geocode_result"], errors="ignore", inplace=True)
        geocoded_df["geocode_result"] = geocoded_df.apply(
            lambda r: "Geocoded" if _has_coords(r) else "Imprecise Geocode",
            axis=1,
        )
        geocoded_df["reason"] = geocoded_df.apply(_reason, axis=1)

        # 4️⃣  tidy-up: remove auxiliary cols (_rid) but KEEP 'address'
        geocoded_df.drop(columns=[c for c in geocoded_df.columns if c.startswith("_")],
                         inplace=True, errors="ignore")
        geocoded_df.drop(columns=merge_cols, inplace=True, errors="ignore")

        # normalise year (no trailing “.0”)
        if "year" in geocoded_df.columns:
            geocoded_df["year"] = pd.to_numeric(
                geocoded_df["year"], errors="coerce"
            ).astype("Int64")

        # 5️⃣  save
        geocoded_df.to_csv(output_file_name, index=False)
        logger.info("geocode_result / reason fixed, _rid removed.")

    except Exception as e:
        logger.warning(f"Could not post-process geocoder output: {e}")

    return os.path.abspath(output_file_name)


#Generate the FIPS code from latitude and longitude
def generate_fips_degauss(df, year, output_folder):
    """
    Generates FIPS codes from latitude and longitude using a Docker-based geocoding service.

    This function saves the provided DataFrame to a CSV file, runs a Docker container to process this file
    and generate FIPS codes, and then updates the DataFrame with the FIPS codes before saving the final output.

    Parameters:
    df (pd.DataFrame): The input DataFrame containing latitude and longitude data.
    year (int): The year to be used for different FIPS code version.

    Returns:
    str or None: The path to the output file if successful, None otherwise.
    """
    output_file = os.path.join(output_folder, f"preprocessed_2_census_block_group_0.6.0_{year}.csv")
    if os.path.exists(output_file):
        logger.info(f"FIPS file already exists, skipping FIPS generation: {output_file}")
        return os.path.abspath(output_file)
    logger.info("Generating FIPS...")

    # Convert the folder and file paths to absolute paths
    abs_output_folder = os.path.abspath(output_folder)  # Convert to absolute path
    
    # Normalize the paths to Unix-style for Docker (replace backslashes with forward slashes)
    abs_output_folder = abs_output_folder.replace("\\", "/")
    
    preprocessed_file_path = os.path.join(output_folder, 'preprocessed_2.csv')
    # Columns that may exist and need to be dropped
    columns_to_drop = {'matched_street', 'matched_zip', 'matched_city', 'matched_state', 'score', 'precision', 'address'}
    # Drop only the columns that exist in the DataFrame
    columns_in_df = set(df.columns)  # Get the columns that exist in the DataFrame
    columns_to_drop = columns_to_drop.intersection(columns_in_df)  # Find intersection of existing columns and columns to drop
    
    if columns_to_drop:  # Only drop if there are columns to drop
        df.drop(columns=columns_to_drop, inplace=True)
        
    #df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'}, inplace=True)
    df = df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon', 'latitude': 'lat', 'longitude' : 'lon'})

    df.to_csv(preprocessed_file_path, index=False)

    # Also normalize the preprocessed file path
    abs_preprocessed_file = os.path.abspath(preprocessed_file_path).replace("\\", "/")
    
    container_cwd = os.getcwd()  # This will be /workspace when using -w /workspace

    # Calculate the relative path from the container's working directory
    host_folder = os.path.join(host_base, os.path.relpath(abs_output_folder, container_cwd))
    #output_file = f"{df.replace('.csv', '')}_census_block_group_0.6.0_{year}.csv"
    mount_dest = 'exposome_tmp'
    docker_command2 = [
        "docker", "run", "--rm",
        "-v", f'{host_folder}:/workspace/{mount_dest}',
        "ghcr.io/degauss-org/census_block_group:0.6.0",
        f'/workspace/{mount_dest}/{os.path.basename(abs_preprocessed_file)}', str(year)
    ]
    logger.debug(f"Mounting host folder to container at /workspace/{mount_dest} instead of /tmp to preserve container /tmp semantics")
    try:
        result = subprocess.run(docker_command2, check=True, capture_output=True, text=True)
        logger.info("Docker command executed successfully.")
        logger.info(result.stdout)
    except subprocess.CalledProcessError as e:
        logger.error(f"Error executing Docker command 2: {e}")
        logger.error(e.stderr)
        return None

    # Define the output file name
    output_file = os.path.join(output_folder, f"preprocessed_2_census_block_group_0.6.0_{year}.csv")
    output_file = os.path.abspath(output_file).replace("\\", "/")

    if os.path.exists(output_file):
        logger.info(f"Output file generated: {output_file}")
        df = pd.read_csv(output_file)
        df['FIPS'] = df[f'census_tract_id_{year}']
        df.drop(columns=[f'census_block_group_id_{year}', f'census_tract_id_{year}'], inplace=True)
        df.to_csv(output_file, index=False)
        return output_file
    else:
        logger.error(f"Expected output file not found: {output_file}")
        return None

# Function to process each individual CSV file
def process_csv_file(file, input_folder, final_coordinate_files):
    file_path = os.path.join(input_folder, file)
    base_filename = os.path.splitext(file)[0]
    output_folder = os.path.join(input_folder, file.replace('.csv', ''))
    os.makedirs(output_folder, exist_ok=True)
    
    encounter_with_fips_file = os.path.join(output_folder, f"{base_filename}_with_fips.csv")
    if os.path.exists(encounter_with_fips_file):
        logger.info(f"Skipping {file}, final FIPS file already exists.")
        return encounter_with_fips_file

    logger.info(f"Processing file: {file_path}")
    df = pd.read_csv(file_path)
    df.rename(columns={col: col.lower().strip() for col in df.columns}, inplace=True)
    if "latitude" in df.columns and "longitude" in df.columns:
        option = 3
        logger.info(f"Detected lat/lon columns in {file}, using option 3.")
    elif "address" in df.columns:
        option = 2
        logger.info(f"Detected address column in {file}, using option 2.")
    elif all(col in df.columns for col in ["street", "city", "state", "zip"]):
        option = 1
        logger.info(f"Detected separate address columns in {file}, using option 1.")
    else:
        logger.error(f"No valid location columns found in {file}, skipping.")
        return None
    
    # Step 1: Check if latitude and longitude are provided (skip geocode if present)
    if option == 3:
        logger.info("Using provided latitude and longitude columns, skipping geocoding.")
        
        logger.info(f"CSV Headers: {df.columns}")
        df.rename(columns={'latitude': 'lat', 'longitude': 'lon'}, inplace=True)

        if "year" in df.columns:
            df['year_for_fips'] = df['year'].apply(lambda x: 2010 if x < 2020 else 2020)
        else:
            logger.error("No valid date column found to infer year.")
            return None

    # Step 2: If latitude and longitude are not provided, check for address columns
    elif option in [1, 2]:
        logger.info("Latitude and longitude not provided. Using address columns for geocoding.")
        threshold = 0.7
        columns = ['street', 'city', 'state', 'zip'] if option == 1 else ['address']
        geocoded_file = generate_coordinates_degauss(df, columns, threshold, output_folder)
        logger.info(f"Geocoded file created: {geocoded_file}")

        # Load geocoded file after processing
        df = pd.read_csv(geocoded_file)
        df['year_for_fips'] = df['year'].apply(lambda x: 2010 if x < 2020 else 2020)

        output_file = os.path.join(output_folder, f"{base_filename}_with_coordinates.csv")
        if not os.path.exists(output_file):
            df.drop(columns=['matched_street', 'matched_zip', 'matched_city', 'matched_state', 'score', 'precision', 'address'], inplace=True, errors='ignore')
            df.rename(columns={'lat': 'latitude', 'lon': 'longitude'}, inplace=True)
            out_df = df.drop(columns=['year_for_fips'], errors='ignore')
            out_df.to_csv(output_file, index=False)
            logger.info(f"Coordinate file saved: {output_file}")
        else:
            logger.info(f"Coordinate file already exists, skipping save: {output_file}")

        final_coordinate_files.append(output_file)

    else:
        logger.error("You must provide either address columns (for geocoding) or latitude and longitude.")
        return None

    # Step 3: Process FIPS generation for 2010 and 2020
    valid_years = [2010, 2020]
    generated_dfs = []

    available_years = set(df['year_for_fips'].unique()) & set(valid_years)
    if not available_years:
        logger.warning("No data available for 2010 or 2020.")
    else:
        for year in sorted(available_years):
            year_df = df[df['year_for_fips'] == year]
            generate_fips_degauss(year_df, year, output_folder)

            fips_file = os.path.join(output_folder, f"preprocessed_2_census_block_group_0.6.0_{year}.csv")
            if os.path.exists(fips_file):
                fips_df = pd.read_csv(fips_file)
                fips_df.drop(columns=['year_for_fips'], inplace=True, errors="ignore")
                fips_df.rename(columns={'lat': 'latitude', 'lon': 'longitude'}, inplace=True)
                generated_dfs.append(fips_df)
            else:
                logger.warning(f"Expected FIPS file missing for year {year}: {fips_file}")

        if generated_dfs:
            final_df = pd.concat(generated_dfs, ignore_index=True) if len(generated_dfs) > 1 else generated_dfs[0]
            final_df.to_csv(encounter_with_fips_file, index=False)
            logger.info(f"Final encounter file with FIPS generated: {encounter_with_fips_file}")
        else:
            logger.error("Error: No FIPS files were generated successfully.")

    try:
        del df
        del year_df
    except NameError:
        pass
    gc.collect()
    return encounter_with_fips_file

        
# Helper: detect native Windows with WSL available
def _is_windows_with_wsl():
    try:
        if platform.system() != "Windows":
            return False
        # wsl.exe should be on PATH for WSL hosts
        return shutil.which("wsl") is not None
    except Exception:
        return False

# Helper: convert a Windows path like C:\Users\x to a WSL mount path /mnt/c/Users/x
def _windows_to_mnt_path(win_path: str) -> str:
    p = os.path.abspath(win_path).replace('\\', '/')
    m = re.match(r'^([A-Za-z]):(.*)$', p)
    if not m:
        # not a drive-prefixed path; return path with forward slashes
        return p
    drive = m.group(1).lower()
    rest = m.group(2)
    # ensure leading slash on rest
    if not rest.startswith('/'):
        rest = '/' + rest
    return f"/mnt/{drive}{rest}"

# Helper: run a command inside WSL bash
def _run_wsl_cmd(cmd: str, check: bool = True):
    # Use bash -lc to ensure proper expansion
    full = ["wsl", "-e", "bash", "-lc", cmd]
    logger.info(f"Running on WSL: {cmd}")
    try:
        result = subprocess.run(full, check=check, capture_output=True, text=True)
        logger.debug(result.stdout)
        if result.stderr:
            logger.debug(result.stderr)
        return result
    except subprocess.CalledProcessError as e:
        logger.error(f"WSL command failed: {e}")
        logger.error(e.stderr)
        if check:
            raise
        return e

def main():
    parser = argparse.ArgumentParser(description='FIPS Geocoding')
    parser.add_argument('-i', '--input', type=str, required=True, help='Input folder path containing CSV files')
    parser.add_argument('--debug', dest='debug', action='store_true', help='Enable debug logging')
    # allow disabling the WSL automation when desired
    parser.add_argument('--no-wsl-auto', dest='no_wsl_auto', action='store_true', help='Do not perform automatic WSL orchestration on native Windows')

    args = parser.parse_args()
    input_folder = args.input

    # If running on native Windows and WSL is present, orchestrate the WSL copy + docker-run + copy-back
    if _is_windows_with_wsl() and not args.no_wsl_auto:
        try:
            win_input_abs = os.path.abspath(input_folder)
            # Create a unique folder name in WSL home to avoid clobbering
            timestamp_short = datetime.now().strftime('%Y%m%d%H%M%S')
            wsl_input_basename = f"input_{timestamp_short}"
            wsl_input_path = f"~/{wsl_input_basename}"

            # Convert Windows input absolute path to WSL /mnt/ path
            wsl_source_dir = _windows_to_mnt_path(win_input_abs)

            # 1) create WSL input dir and copy CSVs into it
            copy_cmd = f"mkdir -p {wsl_input_path} && cp -v {shlex.quote(wsl_source_dir)}/*.csv {wsl_input_path} || true"
            _run_wsl_cmd(copy_cmd)

            # 2) run the Docker container inside WSL mounting the WSL home dir (so the container will see the input folder)
            docker_cmd = (
                f"cd ~ && docker run -it --rm -v \"$(pwd)\":/workspace -v /var/run/docker.sock:/var/run/docker.sock "
                f"-e HOST_PWD=\"$(pwd)\" -w /workspace prismaplab/exposome-geocoder:1.0.2 "
                f"/app/code/Address_to_FIPS.py -i {wsl_input_basename}"
            )
            _run_wsl_cmd(docker_cmd)

            # 3) copy output zip files back from WSL output folder to original Windows working directory
            win_cwd = os.path.abspath(os.getcwd())
            wsl_target_mnt = _windows_to_mnt_path(win_cwd)
            copy_back_cmd = f"mkdir -p {shlex.quote(wsl_target_mnt)} && cp -v ~/output/*.zip {shlex.quote(wsl_target_mnt)}/ || true"
            _run_wsl_cmd(copy_back_cmd)

            logger.info("WSL orchestration complete; outputs copied back to Windows working directory.")
            return
        except Exception as e:
            logger.exception(f"Automatic WSL orchestration failed: {e}")
            logger.warning("Falling back to normal execution path.")

    # If we reach here, either not Windows+WSL or WSL automation disabled/failed — continue normal, cross-platform code
    parent_folder = os.path.dirname(input_folder)
    output_folder = os.path.join(parent_folder, "output")

    #Configure logging to write to the output folder
    configure_logging(output_folder)

    # Generate timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]

    final_fips_files = []  # Collect all final fips files for zipping
    final_coordinate_files = []  # Collect all final coordinate files for zipping

   # Step 1: Use ThreadPoolExecutor to process up to 10 files concurrently
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        futures = {executor.submit(process_csv_file, file, input_folder, final_coordinate_files): file for file in csv_files}

        for future in concurrent.futures.as_completed(futures):
            file = futures[future]
            try:
                result = future.result()
                if result:
                    final_fips_files.append(result)
            except Exception as e:
                logger.error(f"Error processing file {file}: {e}")

    # Step 2: Package all final output files into a zip
    #Create the 'output' folder parallel to the input folder
    
    os.makedirs(output_folder, exist_ok=True)

    zip_filename = os.path.join(output_folder, f"geocoded_fips_codes_{timestamp}.zip")
    
    with zipfile.ZipFile(zip_filename, 'w') as zipf:
        for final_file in final_fips_files:
            if os.path.exists(final_file):  # Only include files that exist
                zipf.write(final_file, os.path.basename(final_file))  # Add the file to the zip archive
            else:
                logger.error(f"Skipping missing file: {final_file}")

    logger.info(f"All output files zipped into: {zip_filename}")

    # Step 3: Package all final coordinate files into a zip
    if final_coordinate_files:  # Check if the list is not empty
        zip_coordinates_filename = os.path.join(output_folder, f"coordinates_from_address_{timestamp}.zip")
    
        with zipfile.ZipFile(zip_coordinates_filename, 'w') as zipf:
            for coord_file in final_coordinate_files:
                if os.path.exists(coord_file):  # Only include files that exist
                    zipf.write(coord_file, os.path.basename(coord_file))  # Add the file to the zip archive
                else:
                    logger.error(f"Skipping missing file: {coord_file}")

        logger.info(f"All coordinate output files zipped into: {zip_coordinates_filename}")
    else:
        logger.warning("No coordinate files to zip. Skipping the creation of coordinates_from_address.zip.")
        
    # Step 4: Remove all the subdirectories, but keep the zip files
    logger.info("Cleaning up subdirectories...")
    for root, dirs, files in os.walk(input_folder, topdown=False):
        for dir_name in dirs:
            dir_path = os.path.join(root, dir_name)
            if os.path.isdir(dir_path):
                shutil.rmtree(dir_path)
                logger.info(f"Deleted directory: {dir_path}")

    logger.info("Cleanup completed. Only zip files and log file remain in the input folder.")



if __name__ == "__main__":
    main()
